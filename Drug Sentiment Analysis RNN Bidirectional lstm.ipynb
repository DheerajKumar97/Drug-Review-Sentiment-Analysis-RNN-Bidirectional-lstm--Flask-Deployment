{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Review Sentiment Analysis\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Sentiment Analysis also known as Opinion Mining refers to the use of natural language processing, text analysis to systematically identify, extract, quantify, and study affective states and subjective information.**\n",
    "\n",
    "**Sentiment analysis is widely applied to reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.**\n",
    "\n",
    "**In this project, we aim to perform Sentiment Analysis of Drug reviews. Data used in this project are online product reviews collected from “amazon.com”. We expect to do review-level categorization of review data with promising outcomes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 =\"E:\\Downlload\\drugsComTest_raw.tsv\"\n",
    "class DataFrame_Loader():\n",
    "\n",
    "    \n",
    "    def __init__(self,error_bad_lines,sep):\n",
    "        self.error_bad_lines = error_bad_lines\n",
    "        self.sep = sep\n",
    "        \n",
    "        print(\"Loadind DataFrame\")\n",
    "        \n",
    "    def load_json_files(self,path1):\n",
    "        dftrain = pd.read_csv(path1,error_bad_lines=True,sep='\\t')\n",
    "        return dftrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind DataFrame\n"
     ]
    }
   ],
   "source": [
    "load = DataFrame_Loader(True,'\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163740</td>\n",
       "      <td>Mirtazapine</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I&amp;#039;ve tried a few antidepressants over th...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>February 28, 2012</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206473</td>\n",
       "      <td>Mesalamine</td>\n",
       "      <td>Crohn's Disease, Maintenance</td>\n",
       "      <td>\"My son has Crohn&amp;#039;s disease and has done ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>May 17, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159672</td>\n",
       "      <td>Bactrim</td>\n",
       "      <td>Urinary Tract Infection</td>\n",
       "      <td>\"Quick reduction of symptoms\"</td>\n",
       "      <td>9.0</td>\n",
       "      <td>September 29, 2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39293</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"Contrave combines drugs that were used for al...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>March 5, 2017</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97768</td>\n",
       "      <td>Cyclafem 1 / 35</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I have been on this birth control for one cyc...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         drugName                     condition  \\\n",
       "0      163740      Mirtazapine                    Depression   \n",
       "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
       "2      159672          Bactrim       Urinary Tract Infection   \n",
       "3       39293         Contrave                   Weight Loss   \n",
       "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"I&#039;ve tried a few antidepressants over th...    10.0   \n",
       "1  \"My son has Crohn&#039;s disease and has done ...     8.0   \n",
       "2                      \"Quick reduction of symptoms\"     9.0   \n",
       "3  \"Contrave combines drugs that were used for al...     9.0   \n",
       "4  \"I have been on this birth control for one cyc...     9.0   \n",
       "\n",
       "                 date  usefulCount  \n",
       "0   February 28, 2012           22  \n",
       "1        May 17, 2009           17  \n",
       "2  September 29, 2017            3  \n",
       "3       March 5, 2017           35  \n",
       "4    October 22, 2015            4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load.load_json_files(path1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "class DataFrame_Preprocessor():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        print(\"Preprocessor object created\")\n",
    "        \n",
    "        \n",
    "    def preprocess(self,df):\n",
    "        \n",
    "        df['Sentiment'] = np.where(df['rating'] > 6, 1, 0)\n",
    "        \n",
    "        df= df[['review','Sentiment']]\n",
    "        \n",
    "        x = df['review']\n",
    "        \n",
    "        y = df['Sentiment']\n",
    "        \n",
    "        return train_test_split(x,y,test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor object created\n"
     ]
    }
   ],
   "source": [
    "PR = DataFrame_Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48389,), (5377,), (48389,), (5377,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = PR.preprocess(df)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Keras Tokenization and Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "class Keras_Tokenizer():\n",
    "\n",
    "    \n",
    "    def __init__(self,max_features):\n",
    "        \n",
    "        self.max_features =6000\n",
    "        \n",
    "        \n",
    "        print(\"Tokenizer object created\")\n",
    "        \n",
    "        \n",
    "    def __label_encoding(self,y_train):\n",
    "        \"\"\"\n",
    "        Encode the given list of class labels\n",
    "        :y_train_enc: returns list of encoded classes\n",
    "        :labels: actual class labels\n",
    "        \"\"\"\n",
    "        lbl_enc = LabelEncoder()\n",
    "\n",
    "        y_train_enc = lbl_enc.fit_transform(y_train)\n",
    "        labels = lbl_enc.classes_\n",
    "\n",
    "        return y_train_enc, labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __word_embedding(self,train, test, max_features, max_len=200):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \"\"\" Keras Tokenizer class object \"\"\"\n",
    "            tokenizer = text.Tokenizer(num_words=max_features)\n",
    "            tokenizer.fit_on_texts(train)\n",
    "\n",
    "            train_data = tokenizer.texts_to_sequences(train)\n",
    "            test_data = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "            \"\"\" Get the max_len \"\"\"\n",
    "            vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "            \"\"\" Padd the sequence based on the max-length \"\"\"\n",
    "            x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n",
    "            x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n",
    "            \"\"\" Return train, test and vocab size \"\"\"\n",
    "            return tokenizer, x_train, x_test, vocab_size\n",
    "        except ValueError as ve:\n",
    "            raise(ValueError(\"Error in word embedding {}\".format(ve)))\n",
    "            \n",
    "            \n",
    "    def preprocess(self,X_train, X_test):\n",
    "        \n",
    "    \n",
    "        return self.__word_embedding(X_train, X_test, self.max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer object created\n"
     ]
    }
   ],
   "source": [
    "KT = Keras_Tokenizer(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48389, 200), (5377, 200), 33068)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pad_train.shape,x_pad_valid.shape,vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling RNN Birectional lstm Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "class RNN_Bidirectional_lstm_Build_Pack():\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_length,\n",
    "                 output_length,\n",
    "                 vocab_size,\n",
    "                 optimizer,\n",
    "                 loss,\n",
    "                 metrics,\n",
    "                 batch_size,\n",
    "                 epochs,\n",
    "                 verbose):\n",
    "        \n",
    "        self.input_length =200\n",
    "        self.output_length= 200\n",
    "        self.vocab_size = 33068\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        self.metrics = ['acc']\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 20\n",
    "        self.verbose = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Tokenizer object created\")\n",
    "        \n",
    "    \n",
    "    def build_rnn(self,vocab_size,output_dim, input_dim):\n",
    "\n",
    "        model = Sequential([\n",
    "            keras.layers.Embedding(self.vocab_size,output_dim = self.output_length,\n",
    "                                  input_length = self.input_length),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True)),\n",
    "            keras.layers.GlobalMaxPool1D(),\n",
    "            keras.layers.Dense(225),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(155),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(150),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(125),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(95),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(34),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(32),\n",
    "            keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def Compile_and_Fit(self,rnn_model):\n",
    "    \n",
    "        rnn_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "\n",
    "\n",
    "        rnn_model.fit(x_pad_train, \n",
    "                                y_train,\n",
    "                                batch_size=self.batch_size,\n",
    "                               epochs=self.epochs,\n",
    "                               verbose= self.verbose)\n",
    "        \n",
    "        score = rnn_model.evaluate(x_pad_valid, y_test, verbose=1)\n",
    "        \n",
    "        print(\"Loss:%.3f Accuracy: %.3f\" % (score[0], score[1]))\n",
    "        \n",
    "        return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer object created\n"
     ]
    }
   ],
   "source": [
    "Rnn_Model = RNN_Bidirectional_lstm_Build_Pack(200,200,33068,'adam','binary_crossentropy',['acc'],256,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 200, 200)          6613600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 200, 512)          935936    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 225)               115425    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 225)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 155)               35030     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 155)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 150)               23400     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 125)               18875     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 95)                11970     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                6144      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 34)                2210      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 32)                1120      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,764,543\n",
      "Trainable params: 7,764,143\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Rnn_Model.build_rnn(vocab_size,1,200)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "190/190 [==============================] - 2624s 14s/step - loss: 0.5382 - acc: 0.7310\n",
      "Epoch 2/20\n",
      "190/190 [==============================] - 1183s 6s/step - loss: 0.3290 - acc: 0.8629\n",
      "Epoch 3/20\n",
      "190/190 [==============================] - 1147s 6s/step - loss: 0.2336 - acc: 0.9086\n",
      "Epoch 4/20\n",
      "190/190 [==============================] - 1814s 10s/step - loss: 0.1291 - acc: 0.9540\n",
      "Epoch 5/20\n",
      "190/190 [==============================] - 1593s 8s/step - loss: 0.0815 - acc: 0.9712\n",
      "Epoch 6/20\n",
      "190/190 [==============================] - 1171s 6s/step - loss: 0.0542 - acc: 0.9813\n",
      "Epoch 7/20\n",
      "190/190 [==============================] - 1889s 10s/step - loss: 0.0436 - acc: 0.9851\n",
      "Epoch 8/20\n",
      "190/190 [==============================] - 1779s 9s/step - loss: 0.0298 - acc: 0.9902\n",
      "Epoch 9/20\n",
      "190/190 [==============================] - 1138s 6s/step - loss: 0.0292 - acc: 0.9895\n",
      "Epoch 10/20\n",
      "190/190 [==============================] - 1143s 6s/step - loss: 0.0274 - acc: 0.9916\n",
      "Epoch 11/20\n",
      "190/190 [==============================] - 1698s 9s/step - loss: 0.0181 - acc: 0.9941\n",
      "Epoch 12/20\n",
      "190/190 [==============================] - 1141s 6s/step - loss: 0.0230 - acc: 0.9921\n",
      "Epoch 13/20\n",
      "190/190 [==============================] - 1960s 10s/step - loss: 0.2850 - acc: 0.9377\n",
      "Epoch 14/20\n",
      "190/190 [==============================] - 2361s 12s/step - loss: 0.0486 - acc: 0.9873\n",
      "Epoch 15/20\n",
      "190/190 [==============================] - 1133s 6s/step - loss: 0.0143 - acc: 0.9958\n",
      "Epoch 16/20\n",
      "190/190 [==============================] - 1134s 6s/step - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 17/20\n",
      "190/190 [==============================] - 2232s 12s/step - loss: 0.0041 - acc: 0.9994\n",
      "Epoch 18/20\n",
      "190/190 [==============================] - 1970s 10s/step - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 19/20\n",
      "190/190 [==============================] - 1141s 6s/step - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 20/20\n",
      "190/190 [==============================] - 1866s 10s/step - loss: 0.0249 - acc: 0.9932\n",
      "169/169 [==============================] - 87s 515ms/step - loss: 0.9724 - acc: 0.8590\n",
      "Loss:0.972 Accuracy: 0.859\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Rnn_Model.Compile_and_Fit(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds Shape :: (5377, 1)\n",
      "(5377, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred\n",
       "0     1\n",
       "1     1\n",
       "2     0\n",
       "3     1\n",
       "4     1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = rnn_model.predict(x_pad_valid)\n",
    "\n",
    "print(\"y_preds Shape ::\",y_preds.shape)\n",
    "\n",
    "\n",
    "for arr in y_preds:\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]>0.5:\n",
    "            arr[i] = 1\n",
    "        else:\n",
    "            arr[i] = 0\n",
    "\n",
    "            \n",
    "y_preds = y_preds.astype('int32')\n",
    "\n",
    "pred_df = pd.DataFrame(y_preds, columns=['pred'])\n",
    "\n",
    "print(pred_df.shape)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "1       3637\n",
       "0       1740\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8590291984377906\n",
      "[[1424  442]\n",
      " [ 316 3195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.76      0.79      1866\n",
      "           1       0.88      0.91      0.89      3511\n",
      "\n",
      "    accuracy                           0.86      5377\n",
      "   macro avg       0.85      0.84      0.84      5377\n",
      "weighted avg       0.86      0.86      0.86      5377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.accuracy_score(y_test, pred_df))\n",
    "        \n",
    "print(metrics.confusion_matrix(y_test, pred_df))\n",
    "        \n",
    "print(metrics.classification_report(y_test, pred_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = rnn_model.to_json()\n",
    "with open(\"rnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "rnn_model.save(\"rnn_model.h5\", overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('test_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
