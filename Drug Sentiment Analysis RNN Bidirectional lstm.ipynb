{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Review Sentiment Analysis\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Sentiment Analysis also known as Opinion Mining refers to the use of natural language processing, text analysis to systematically identify, extract, quantify, and study affective states and subjective information.**\n",
    "\n",
    "**Sentiment analysis is widely applied to reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.**\n",
    "\n",
    "**In this project, we aim to perform Sentiment Analysis of Drug reviews. Data used in this project are online product reviews collected from “amazon.com”. We expect to do review-level categorization of review data with promising outcomes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 =\"E:\\Downlload\\drugsComTest_raw.tsv\"\n",
    "class DataFrame_Loader():\n",
    "\n",
    "    \n",
    "    def __init__(self,error_bad_lines,sep):\n",
    "        self.error_bad_lines = error_bad_lines\n",
    "        self.sep = sep\n",
    "        \n",
    "        print(\"Loadind DataFrame\")\n",
    "        \n",
    "    def load_data_files(self,path1):\n",
    "        dftrain = pd.read_csv(path1,error_bad_lines=True,sep='\\t')\n",
    "        return dftrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind DataFrame\n"
     ]
    }
   ],
   "source": [
    "load = DataFrame_Loader(True,'\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163740</td>\n",
       "      <td>Mirtazapine</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I&amp;#039;ve tried a few antidepressants over th...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>February 28, 2012</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206473</td>\n",
       "      <td>Mesalamine</td>\n",
       "      <td>Crohn's Disease, Maintenance</td>\n",
       "      <td>\"My son has Crohn&amp;#039;s disease and has done ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>May 17, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159672</td>\n",
       "      <td>Bactrim</td>\n",
       "      <td>Urinary Tract Infection</td>\n",
       "      <td>\"Quick reduction of symptoms\"</td>\n",
       "      <td>9.0</td>\n",
       "      <td>September 29, 2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39293</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"Contrave combines drugs that were used for al...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>March 5, 2017</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97768</td>\n",
       "      <td>Cyclafem 1 / 35</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I have been on this birth control for one cyc...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         drugName                     condition  \\\n",
       "0      163740      Mirtazapine                    Depression   \n",
       "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
       "2      159672          Bactrim       Urinary Tract Infection   \n",
       "3       39293         Contrave                   Weight Loss   \n",
       "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"I&#039;ve tried a few antidepressants over th...    10.0   \n",
       "1  \"My son has Crohn&#039;s disease and has done ...     8.0   \n",
       "2                      \"Quick reduction of symptoms\"     9.0   \n",
       "3  \"Contrave combines drugs that were used for al...     9.0   \n",
       "4  \"I have been on this birth control for one cyc...     9.0   \n",
       "\n",
       "                 date  usefulCount  \n",
       "0   February 28, 2012           22  \n",
       "1        May 17, 2009           17  \n",
       "2  September 29, 2017            3  \n",
       "3       March 5, 2017           35  \n",
       "4    October 22, 2015            4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load.load_data_files(path1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrame_Preprocessor():\n",
    "\n",
    "    \n",
    "    def __init__(self,n_rare_words):\n",
    "        self.n_rare_words = 10\n",
    "        \n",
    "        \n",
    "        print(\"Preprocessor object created\")\n",
    "        \n",
    "\n",
    "    def __remove_punctuation(self,text):\n",
    "        \n",
    "        PUNCT_TO_REMOVE = string.punctuation\n",
    "        \"\"\"custom function to remove the punctuation\"\"\"\n",
    "        return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "    def __remove_stopwords(self,text):\n",
    "        \n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        \"\"\"custom function to remove the stopwords\"\"\"\n",
    "        return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "    def Get_Most_Commom(self,data):\n",
    "        \n",
    "        cnt = Counter()\n",
    "        for text in df[\"review\"].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "\n",
    "        return cnt.most_common(10)\n",
    "\n",
    "\n",
    "    def __remove_freqwords(self,text):\n",
    "        \n",
    "        FREQWORDS = set([w for (w, wc) in count])\n",
    "        \"\"\"custom function to remove the frequent words\"\"\"\n",
    "        return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "    \n",
    "    def __remove_rarewords(self,text):\n",
    "        \n",
    "        RAREWORDS = set([w for (w, wc) in count[:-self.n_rare_words-1:-1]])\n",
    "        \"\"\"custom function to remove the rare words\"\"\"\n",
    "        return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "    \n",
    "    def __stem_words(self,text):\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    def Text_Preprocessing(self,data):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            data = data[['review','rating']]\n",
    "            data[\"review\"] = data[\"review\"].apply(lambda text: self.__remove_punctuation(text))\n",
    "            data[\"review\"] = data[\"review\"].apply(lambda text: self.__remove_stopwords(text))\n",
    "            data[\"review\"] = data[\"review\"].apply(lambda text: self.__remove_freqwords(text))\n",
    "            data[\"review\"] = data[\"review\"].apply(lambda text: self.__remove_rarewords(text))\n",
    "            data[\"review\"] = data[\"review\"].apply(lambda text: self.__stem_words(text))\n",
    "            data = data.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "            data['review'] = data['review'].str.replace('\\d+', '')\n",
    "            return data\n",
    "        \n",
    "        except ValueError as ve:\n",
    "            raise(ValueError(\"Error in Text Preprocessing {}\".format(ve)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor object created\n"
     ]
    }
   ],
   "source": [
    "preprocess = DataFrame_Preprocessor(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 220967),\n",
       " ('and', 140452),\n",
       " ('the', 121334),\n",
       " ('to', 106483),\n",
       " ('a', 94216),\n",
       " ('my', 82172),\n",
       " ('it', 66437),\n",
       " ('for', 64868),\n",
       " ('was', 57513),\n",
       " ('of', 56438)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = preprocess.Get_Most_Commom(df)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32129</th>\n",
       "      <td>cold sore outbreak  xyear last  year usual sum...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46883</th>\n",
       "      <td>mirena go  year great put son born  didnt feel...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>overal excel medicin sore inject site water re...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50204</th>\n",
       "      <td>quit bit stress job relat put mg day dose work...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26477</th>\n",
       "      <td>iv birth control sinc junior high school attem...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review rating\n",
       "32129  cold sore outbreak  xyear last  year usual sum...   10.0\n",
       "46883  mirena go  year great put son born  didnt feel...   10.0\n",
       "9002   overal excel medicin sore inject site water re...    7.0\n",
       "50204  quit bit stress job relat put mg day dose work...   10.0\n",
       "26477  iv birth control sinc junior high school attem...    5.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess.Text_Preprocessing(df)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "class DataFrame_Preprocessor():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        print(\"Preprocessor object created\")\n",
    "        \n",
    "        \n",
    "    def preprocess(self,data):\n",
    "        \n",
    "        data['rating'] = pd.to_numeric(data['rating'],errors='coerce')\n",
    "        \n",
    "        data['Sentiment'] = np.where(data['rating'] > 6, 1, 0)\n",
    "        \n",
    "        data= data[['review','Sentiment']]\n",
    "        \n",
    "        x = data['review']\n",
    "        \n",
    "        y = data['Sentiment']\n",
    "        \n",
    "        return train_test_split(x,y,test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor object created\n"
     ]
    }
   ],
   "source": [
    "PR = DataFrame_Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48389,), (5377,), (48389,), (5377,))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = PR.preprocess(df)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Keras Tokenization and Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "class Keras_Tokenizer():\n",
    "\n",
    "    \n",
    "    def __init__(self,max_features):\n",
    "        \n",
    "        self.max_features =6000\n",
    "        \n",
    "        \n",
    "        print(\"Tokenizer object created\")\n",
    "        \n",
    "        \n",
    "    def __label_encoding(self,y_train):\n",
    "        \"\"\"\n",
    "        Encode the given list of class labels\n",
    "        :y_train_enc: returns list of encoded classes\n",
    "        :labels: actual class labels\n",
    "        \"\"\"\n",
    "        lbl_enc = LabelEncoder()\n",
    "\n",
    "        y_train_enc = lbl_enc.fit_transform(y_train)\n",
    "        labels = lbl_enc.classes_\n",
    "\n",
    "        return y_train_enc, labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __word_embedding(self,train, test, max_features, max_len=200):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \"\"\" Keras Tokenizer class object \"\"\"\n",
    "            tokenizer = text.Tokenizer(num_words=max_features)\n",
    "            tokenizer.fit_on_texts(train)\n",
    "\n",
    "            train_data = tokenizer.texts_to_sequences(train)\n",
    "            test_data = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "            \"\"\" Get the max_len \"\"\"\n",
    "            vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "            \"\"\" Padd the sequence based on the max-length \"\"\"\n",
    "            x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n",
    "            x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n",
    "            \"\"\" Return train, test and vocab size \"\"\"\n",
    "            return tokenizer, x_train, x_test, vocab_size\n",
    "        except ValueError as ve:\n",
    "            raise(ValueError(\"Error in word embedding {}\".format(ve)))\n",
    "            \n",
    "            \n",
    "    def preprocess(self,X_train, X_test):\n",
    "        \n",
    "    \n",
    "        return self.__word_embedding(X_train, X_test, self.max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer object created\n"
     ]
    }
   ],
   "source": [
    "KT = Keras_Tokenizer(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, x_pad_train, x_pad_valid, vocab_size = KT.preprocess(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48389, 200), (5377, 200), 37436)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pad_train.shape,x_pad_valid.shape,vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling RNN Birectional lstm Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "class RNN_Bidirectional_lstm_Build_Pack():\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_length,\n",
    "                 output_length,\n",
    "                 vocab_size,\n",
    "                 optimizer,\n",
    "                 loss,\n",
    "                 metrics,\n",
    "                 batch_size,\n",
    "                 epochs,\n",
    "                 verbose):\n",
    "        \n",
    "        self.input_length =200\n",
    "        self.output_length= 200\n",
    "        self.vocab_size = 33068\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        self.metrics = ['acc']\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 20\n",
    "        self.verbose = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Tokenizer object created\")\n",
    "        \n",
    "    \n",
    "    def build_rnn(self,vocab_size,output_dim, input_dim):\n",
    "\n",
    "        model = Sequential([\n",
    "            keras.layers.Embedding(self.vocab_size,output_dim = self.output_length,\n",
    "                                  input_length = self.input_length),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True)),\n",
    "            keras.layers.GlobalMaxPool1D(),\n",
    "            keras.layers.Dense(225,activation='relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(150,activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(95,activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64,activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(34,activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(32,activation='relu'),\n",
    "            keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def Compile_and_Fit(self,rnn_model):\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            rnn_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "\n",
    "\n",
    "            rnn_model.fit(x_pad_train, \n",
    "                                    y_train,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                   epochs=self.epochs,\n",
    "                                   verbose= self.verbose)\n",
    "\n",
    "            score = rnn_model.evaluate(x_pad_valid, y_test, verbose=1)\n",
    "\n",
    "            print(\"Loss:%.3f Accuracy: %.3f\" % (score[0], score[1]))\n",
    "\n",
    "            return rnn_model\n",
    "        \n",
    "        except ValueError as Model_Error:\n",
    "            raise(ValueError(\"Model Compiling Error {}\".format(Model_Error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer object created\n"
     ]
    }
   ],
   "source": [
    "Rnn_Model = RNN_Bidirectional_lstm_Build_Pack(200,200,33068,'adam','binary_crossentropy',['acc'],256,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 200)          6613600   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 512)          935936    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 225)               115425    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 225)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               33900     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 95)                14345     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 95)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                6144      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 34)                2210      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1120      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,723,513\n",
      "Trainable params: 7,723,113\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Rnn_Model.build_rnn(vocab_size,1,200)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "190/190 [==============================] - 4286s 23s/step - loss: 0.5077 - acc: 0.7534\n",
      "Epoch 2/20\n",
      "190/190 [==============================] - 1313s 7s/step - loss: 0.3179 - acc: 0.8685\n",
      "Epoch 3/20\n",
      "190/190 [==============================] - 1254s 7s/step - loss: 0.2137 - acc: 0.9185\n",
      "Epoch 4/20\n",
      "190/190 [==============================] - 1218s 6s/step - loss: 0.1205 - acc: 0.9560\n",
      "Epoch 5/20\n",
      "190/190 [==============================] - 2800s 15s/step - loss: 0.0816 - acc: 0.9711\n",
      "Epoch 6/20\n",
      "190/190 [==============================] - 2402s 13s/step - loss: 0.0371 - acc: 0.9871\n",
      "Epoch 7/20\n",
      "190/190 [==============================] - 2308s 12s/step - loss: 0.0552 - acc: 0.9797\n",
      "Epoch 8/20\n",
      "190/190 [==============================] - 1430s 8s/step - loss: 0.0280 - acc: 0.9903\n",
      "Epoch 9/20\n",
      "190/190 [==============================] - 1255s 7s/step - loss: 0.0201 - acc: 0.9936\n",
      "Epoch 10/20\n",
      "190/190 [==============================] - 1244s 7s/step - loss: 0.0176 - acc: 0.9938\n",
      "Epoch 11/20\n",
      "190/190 [==============================] - 1246s 7s/step - loss: 0.0209 - acc: 0.9937\n",
      "Epoch 12/20\n",
      "190/190 [==============================] - 1727s 9s/step - loss: 0.0174 - acc: 0.9944\n",
      "Epoch 13/20\n",
      "190/190 [==============================] - 2291s 12s/step - loss: 0.0176 - acc: 0.9940\n",
      "Epoch 14/20\n",
      "190/190 [==============================] - 1200s 6s/step - loss: 0.0256 - acc: 0.9912\n",
      "Epoch 15/20\n",
      "190/190 [==============================] - 3681s 19s/step - loss: 0.0177 - acc: 0.9940\n",
      "Epoch 16/20\n",
      "190/190 [==============================] - 1205s 6s/step - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 17/20\n",
      "190/190 [==============================] - 1200s 6s/step - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 18/20\n",
      "190/190 [==============================] - 2810s 15s/step - loss: 0.0173 - acc: 0.9942\n",
      "Epoch 19/20\n",
      "190/190 [==============================] - 1204s 6s/step - loss: 0.0231 - acc: 0.9917\n",
      "Epoch 20/20\n",
      "190/190 [==============================] - 1745s 9s/step - loss: 0.0131 - acc: 0.9959\n",
      "169/169 [==============================] - 94s 554ms/step - loss: 0.8024 - acc: 0.8603\n",
      "Loss:0.802 Accuracy: 0.860\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Rnn_Model.Compile_and_Fit(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds Shape :: (5377, 1)\n",
      "(5377, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred\n",
       "0     1\n",
       "1     1\n",
       "2     0\n",
       "3     1\n",
       "4     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = rnn_model.predict(x_pad_valid)\n",
    "\n",
    "print(\"y_preds Shape ::\",y_preds.shape)\n",
    "\n",
    "\n",
    "for arr in y_preds:\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]>0.5:\n",
    "            arr[i] = 1\n",
    "        else:\n",
    "            arr[i] = 0\n",
    "\n",
    "            \n",
    "y_preds = y_preds.astype('int32')\n",
    "\n",
    "pred_df = pd.DataFrame(y_preds, columns=['pred'])\n",
    "\n",
    "print(pred_df.shape)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "1       3528\n",
       "0       1849\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603310396131671\n",
      "[[1482  384]\n",
      " [ 367 3144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80      1866\n",
      "           1       0.89      0.90      0.89      3511\n",
      "\n",
      "    accuracy                           0.86      5377\n",
      "   macro avg       0.85      0.84      0.85      5377\n",
      "weighted avg       0.86      0.86      0.86      5377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.accuracy_score(y_test, pred_df))\n",
    "        \n",
    "print(metrics.confusion_matrix(y_test, pred_df))\n",
    "        \n",
    "print(metrics.classification_report(y_test, pred_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save(\"rnn_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
